{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Amara was here\n",
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Bob Barker:  Analysis, Software, Visualization\n",
    "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
    "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In New York City, how does taxi and ride hailing activity (Uber and Lyft proxy via TLC HVFHV trips) affect the nearest MTA subway station ridership (total riders per day) on a daily level from March to November 2025, controlling for trends such as seasonality, weather, and gas prices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Public transit agencies have pursued service expansions and frequency improvements to attract riders, yet many U.S. cities experienced stagnation or decline in ridership in the years leading up to COVID-19. A Congressional Research Service review notes that despite major capital investments (including rail expansions), overall ridership fell in many large markets, and new rail lines do not necessarily increase total system ridership– sometimes shifting resources away from bus networks that carry a large share of trips.<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup> At the same time, multiple competitive and behavioral forces changed the travel market: cheaper gasoline, rising fares, telework, and the growth of alternative modes like bikeshare and ride-hailing.<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>\n",
    "\n",
    "## Prior work\n",
    "Recent empirical work suggests that transit supply improvements can raise ridership, but the net outcome depends on the broader context. Erhardt et al. (2022) quantify ridership change across U.S. metropolitan areas pre-pandemic and find that while expanded transit service and land-use changes would have increased ridership, these gains were outweighed by other factors—especially ride-hailing along with higher fares, higher incomes, more teleworking, higher car ownership, and lower gas prices.<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup> Complementing this, Graehler et al. (TRB 2019) document mode-specific ridership declines in major U.S. cities since the mid-2010s and enumerate commonly cited explanations including service cuts, deferred-maintenance reliability issues, micro mobility growth, and Transportation Network Company(TNC) expansion(Uber/Lyft); their panel analysis finds that standard factors (service levels, gas price, auto ownership) do not fully explain the recent declines and reports negative associations between years-since-TNC entry and bus/heavy-rail ridership.<sup><a href=\"#fn2\" id=\"ref2\">2</a></sup>\n",
    "\n",
    "## Motivation for this project\n",
    "While “increase frequency” is a widely recommended lever, agencies operate under budget constraints where additional service hours have opportunity costs. Tabassum et al. (2025) identify strategies used by 12 “successful” U.S. agencies that grew ridership from 2010–2019, including service expansion and improvements(beyond frequency including cleanliness and timeliness) among other supporting policies (e.g., pricing/parking constraints, targeted incentives, outreach).<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup> This motivates a productivity-focused question: whether there exists a practical headway threshold (e.g., ≤15 minutes) after which additional frequency yields diminishing returns when evaluated as ridership per unit of supplied service. Building on the above literature, we will test for non-linear (threshold) relationships between frequency/headway and a service-productivity metric (e.g., riders per service hour: Passenger Miles Traveled(PMT)/Vehicle Revenue Hours(VRH)), while accounting for confounders such as car ownership, fare changes, and the presence of ride-hailing and other competing modes.<sup><a href=\"#fn3\" id=\"ref3\">3</a></sup>\n",
    "\n",
    "\n",
    "## References (Footnotes)\n",
    "\n",
    "<p id=\"fn1\">\n",
    "1. Mallett, W. J. (2018). <i>Trends in Public Transportation Ridership: Implications for Federal Policy</i> (CRS Report R45144). Congressional Research Service.\n",
    "<a href=\"https://www.congress.gov/crs_external_products/R/PDF/R45144/R45144.3.pdf\">https://www.congress.gov/crs_external_products/R/PDF/R45144/R45144.3.pdf</a>\n",
    "<a href=\"#ref1\">^</a>\n",
    "</p>\n",
    "\n",
    "<p id=\"fn2\">\n",
    "2. Erhardt, G. D., et al. (2022). Why has public transit ridership declined in the United States?\n",
    "<i>Transportation Research Part A</i>, 161, 68–87.\n",
    "<a href=\"https://www.sciencedirect.com/science/article/pii/S0965856422000945\">https://www.sciencedirect.com/science/article/pii/S0965856422000945</a>\n",
    "<a href=\"#ref2\">^</a>\n",
    "</p>\n",
    "\n",
    "<p id=\"fn3\">\n",
    "3. Tabassum, N., et al. (2025). Ways of increasing transit ridership—lessons learned from successful transit agencies.\n",
    "<i>Case Studies on Transport Policy</i>, 19.\n",
    "<a href=\"https://www.sciencedirect.com/science/article/pii/S2213624X24002177\">https://www.sciencedirect.com/science/article/pii/S2213624X24002177</a>\n",
    "<a href=\"#ref3\">^</a>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After accounting for trends (seasonality, weather, and gas prices), stations with high surrounding ride hailing activity will have a lower number of riders per day. We anticipate that during more intense weather conditions (heat, cold, rain, and snow), subway stations will experience less ridership and ride hailing activity will increase accordingly. Furthermore, we anticipate as gas prices increase, ridesharing prices will follow suit, lowering their ridership, and increasing subway ridership.\n",
    "\n",
    "The hypothesized thresholds are as follows:\n",
    "\n",
    "- Temperature (on days when it is not raining or snowing): \n",
    "    - Lower threshold: 50°F (10°C) (temperatures below this will lead to a decrease in subway ridership)\n",
    "    - Upper threshold: 80°F (27°C) (temperatures above this will lead to a decrease in subway ridership)\n",
    "- Rain: rain for over 1/3 of the day will see a decrease in ridership\n",
    "- Snow: any amount of snow will lead to a decrease in ridership\n",
    "- Gas Prices: $3.50 (gas prices exceeding this amount will lead to increased subway ridership)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data proposal (Updated Project Proposal)\n",
    "1. The ideal datasets include:\n",
    "   1. A dataset on NYC subways including variables: date, station (or station longitude and latitude), and total number of riders that day. Observations should be daily at minimum and over the span of a few months to factor in seasonality. This data would be collected by MTA, the system for public transportation in NYC. This data will be stored locally.\n",
    "   2. A dataset on NYC ride-hailing trips (Uber and Lyft) with variables: day, pick up longitude and latitude, drop off longitude and latitude, and fare. This should also be daily observations over the span of a few months (same months as subway dataset). This data is collected by TLC, the Taxi & Limousine Commission in NYC, in their HVFHV (High Volume For Higher Vehicles) dataset. This data will also be stored locally.\n",
    "   3. A dataset on NYC daily weather conditions including variables: day, highest daily temperature, lowest daily temperature, average daily temperature, rain, and snow. Daily observations are needed, spanning the same months as subway and ride hailing data. This data is collected by the NCDC, National Centers for Environmental Information, and will also be stored locally.\n",
    "   4. A dataset on NYC's daily gas prices including variables: day, average gas price. Observations should be daily and span the same months as previous listed data. This data is collected by the EIA, Energy Information Administration, and will also be stored locally.\n",
    "\n",
    "2. Datasets we will use include:\n",
    "    1. MTA: NYC Subway Ridership 2025 (https://data.ny.gov/Transportation/MTA-Subway-Hourly-Ridership-Beginning-2025/5wq4-mkjj/about_data). The data is located in the official open data portal for NYC. It is freely available to read and use. Variables include: transit_timestamp, transit_mode, station_complex_id, station_complex, ridership, latitude, longitude.\n",
    "    2. TLC: NYC HVFHV 2009-2025 (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) The data is located on the official website of the NYC government and is freely available to read and use. Variables include: pickup_datetime, dropoff_datetime, PULocationID (TLC Taxi Zone where the trip began), DOLocationID (TLC Taxi Zone where trip ended), trip_time, base_passenger_fare, bcf (amount collected in Black Car Fund), sales_tax, congestion_surcharge, airport_fee, tips, cbd_congestion_fee. Fees and taxes are included to calculate total fare prices.\n",
    "    3. NCDC-NOAA: NYC Daily Summaries for NYC 1843-2026 (https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/locations/CITY:US360019/detail). The data is located on the National Centers for Environmental Information website and is freely available to request and use. Variables include: year, month, day, temperature_max, temperature_min, precipitation.\n",
    "    4. EIA Weekly NYC Retail Gasoline Prices 2000-2026 (https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=EMM_EPMR_PTE_Y35NY_DPG&f=W) The data is located in the U.S. Energy Information Administration (EIA) government website and is freely available to read and use. Variables include: first_day_of_week, dollars_per_gallon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://data.cityofnewyork.us/api/v3/views/u253-aew4/query.csv?query=SELECT%0A%20%20%60pickup_datetime%60%2C%0A%20%20%60dropoff_datetime%60%2C%0A%20%20%60pulocationid%60%2C%0A%20%20%60dolocationid%60%2C%0A%20%20%60trip_miles%60%2C%0A%20%20%60trip_time%60%2C%0A%20%20%60base_passenger_fare%60%2C%0A%20%20%60congestion_surcharge%60%2C%0A%20%20%60airport_fee%60%2C%0A%20%20%60tips%60%2C%0A%20%20%60driver_pay%60%2C%0A%20%20%60hvfhs_license_num%60%2C%0A%20%20%60sales_tax%60%2C%0A%20%20%60tolls%60%0AWHERE%0A%20%20%60request_datetime%60%0A%20%20%20%20BETWEEN%20%222023-02-18T19%3A21%3A09%22%20%3A%3A%20floating_timestamp%0A%20%20%20%20AND%20%222023-02-19T19%3A21%3A09%22%20%3A%3A%20floating_timestamp', 'filename':'HVFHV_Trip_Data_20230218_incomplete.csv'},\n",
    "    { 'url': 'https://data.ny.gov/api/v3/views/wujg-7c2s/query.csv?query=SELECT%0A%20%20%60transit_timestamp%60%2C%0A%20%20%60transit_mode%60%2C%0A%20%20%60station_complex_id%60%2C%0A%20%20%60borough%60%2C%0A%20%20%60ridership%60%2C%0A%20%20%60latitude%60%2C%0A%20%20%60longitude%60%2C%0A%20%20%60georeference%60%0AWHERE%0A%20%20%60transit_timestamp%60%0A%20%20%20%20BETWEEN%20%222023-02-18T19%3A21%3A09%22%20%3A%3A%20floating_timestamp%0A%20%20%20%20AND%20%222023-02-19T19%3A21%3A09%22%20%3A%3A%20floating_timestamp%0AORDER%20BY%20%60transit_timestamp%60%20ASC%20NULL%20LAST', 'filename':'MTS_Trip_Data_20230218_incomplete.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sodapy in /home/dum001/.local/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests>=2.28.1 in /home/dum001/.local/lib/python3.11/site-packages (from sodapy) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "The total number of missing values: 0\n",
      "             transit_timestamp transit_mode station_complex_id  \\\n",
      "0      2025-03-18T02:00:00.000       subway                619   \n",
      "1      2025-03-18T02:00:00.000       subway                612   \n",
      "2      2025-03-18T04:00:00.000       subway                624   \n",
      "3      2025-03-18T05:00:00.000       subway                326   \n",
      "4      2025-03-18T06:00:00.000       subway                143   \n",
      "...                        ...          ...                ...   \n",
      "29995  2025-03-21T17:00:00.000       subway                308   \n",
      "29996  2025-03-21T17:00:00.000       subway                397   \n",
      "29997  2025-03-21T18:00:00.000       subway                  8   \n",
      "29998  2025-03-21T18:00:00.000       subway                476   \n",
      "29999  2025-03-21T18:00:00.000       subway                440   \n",
      "\n",
      "                                         station_complex    borough ridership  \\\n",
      "0        Broadway-Lafayette St (B,D,F,M)/Bleecker St (6)  Manhattan       9.0   \n",
      "1                     Lexington Av-53 St (E,M)/51 St (6)  Manhattan      27.0   \n",
      "2      Chambers St (A,C)/WTC (E)/Park Pl (2,3)/Cortla...  Manhattan       3.0   \n",
      "3                                        Franklin St (1)  Manhattan       1.0   \n",
      "4                                      Inwood-207 St (A)  Manhattan      30.0   \n",
      "...                                                  ...        ...       ...   \n",
      "29995                        Cathedral Pkwy (110 St) (1)  Manhattan      74.0   \n",
      "29996                                      86 St (4,5,6)  Manhattan     263.0   \n",
      "29997                                 5 Av/59 St (N,R,W)  Manhattan     137.0   \n",
      "29998                                          86 St (Q)  Manhattan     121.0   \n",
      "29999                                       116 St (2,3)  Manhattan      13.0   \n",
      "\n",
      "        latitude   longitude  \n",
      "0      40.725914   -73.99466  \n",
      "1      40.757553  -73.969055  \n",
      "2       40.71411   -74.00858  \n",
      "3       40.71932   -74.00689  \n",
      "4      40.868073    -73.9199  \n",
      "...          ...         ...  \n",
      "29995  40.803967   -73.96685  \n",
      "29996   40.77949   -73.95559  \n",
      "29997  40.764812   -73.97335  \n",
      "29998   40.77789   -73.95179  \n",
      "29999  40.802097   -73.94962  \n",
      "\n",
      "[30000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import seaborn and apply its plotting styles\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2, style=\"white\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# For MTA Dataset import\n",
    "%pip install sodapy\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.ny.gov\", None)\n",
    "\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "# a WARNNING will occur, this is expected.\n",
    "results = client.get(\"5wq4-mkjj\", \n",
    "                     where=\"transit_mode = 'subway' AND borough = 'Manhattan'\", \n",
    "                     limit=30000)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_hr_subway = pd.DataFrame.from_records(results)\n",
    "\n",
    "# Get the total count of missing values in the whole DataFrame\n",
    "print(\"The total number of missing values:\", df_hr_subway.isna().sum().sum())\n",
    "\n",
    "# Drop multiple columns using a list\n",
    "df_hr_subway = df_hr_subway.drop(['payment_method', 'fare_class_category', 'transfers', 'georeference'], axis=1)\n",
    "\n",
    "df_hr_subway.ridership.value_counts()\n",
    "\n",
    "print(df_hr_subway)\n",
    "\n",
    "df_hr_subway.to_csv(\"data/02-processed/df_hr_subway.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "hvfhv_df = pd.read_csv(\"data/00-raw/2023_High_Volume_FHV_Trip_Data_20260218.csv\")\n",
    "print(\"data loaded successfully\")\n",
    "print(hvfhv_df.columns)\n",
    "\n",
    "#add all the costs together to get the total cost of the trip\n",
    "hvfhv_df['total_cost'] = hvfhv_df['base_passenger_fare'] + hvfhv_df['congestion_surcharge'] + hvfhv_df['airport_fee'] + hvfhv_df['tips'] + hvfhv_df['tolls'] + hvfhv_df['sales_tax']\n",
    "#drop the columns that display the cost breakdown and keep only the total cost column\n",
    "df_new = hvfhv_df.drop(columns=['base_passenger_fare', 'congestion_surcharge', 'airport_fee', 'tips', 'tolls', 'sales_tax'])\n",
    "#drop the other unnecessary columns\n",
    "df_new = df_new.drop(columns= ['dropoff_datetime', 'DOLocationID', 'driver_pay', 'hvfhs_license_num'])\n",
    "\n",
    "\n",
    "print(df_new.isna().sum())\n",
    "#there are no missing values in the columns we kepts\n",
    "\n",
    "print(df_new['total_cost'].max())\n",
    "print(df_new['trip_miles'].max())\n",
    "#Although there are some trips that are very expensive, the maximum trip cost isn't an outlier\n",
    "#Although there are some trips that are very long, the maximum trip distance isn't an outlier\n",
    "print(df_new['total_cost'].median())\n",
    "print(df_new['trip_miles'].median())\n",
    "\n",
    "print(df_new.describe())\n",
    "#create new csv\n",
    "df_new.to_csv(\"data/02-processed/2023_High_Volume_FHV_Trip_Data_20260218_cleaned.csv\", index=False)\n",
    "print(df_new.columns)\n",
    "print(df_new.head())\n",
    "print(\"data cleaned and saved successfully\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "> The datasets we are planning to use are directly from the public transit agencies. Due to the data being transit operations rather than individual user behavior, the traditional need for consent is not applicable. So I would say that the public transit riders may implicitly consent to data collection through the transit systems terms of use\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "> Collection biases that we may come across are those who evade transportation fares. They would not be counted in the ridership counts. However, I believe that this group would be negligible when compared to the overall ridership population. With this in mind, conclusions will always be estimations. Bias’s that would matter more is at what time the data is taken, COVID era would be an outlier due to the changes during that year. Along with bus ridership during school and when school is out would affect the ridership counts.\n",
    "\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "> The data set that we are using does not have any personally identifying information. It consists of information such as aggregate ridership counts and schedule data.\n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "> The dataset does not include any demographic information such as race, gender, or socioeconomic status. Which would limit our ability to mitigate or even test for those factors, but also means that sensitive information is not at risk.\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "> The data is from public agencies and public accessible and contains no personally identifiable information, there is no need for more security measures beyond ethically using the data.\n",
    "\n",
    " - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "> To reduce any blind spots, we are planning on drawing from multiple sources across agencies that do transportation in San Diego, which consist of San Diego County Metropolitan Transit System (SDMTS), North County Transit District (NCTD) and more. Using multiple sources will help validate findings and capture the bigger picture\n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "> We know that there is a risk of our sources being biased which would make our data biased. Socioeconomic factors could influence ridership patterns, as a more transit-dependent community may respond differently to frequency changes than less dependent riders. If fare data is involved, we can normalize based on the area’s cost of living. Another factor to consider is the more heavily populated areas and high traffic places, may generate more ridership data. We will do our best to account for this by avoiding over generalizing any findings especially to those highly dense routes.\n",
    "\n",
    " - [ ] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    "\n",
    ">Data with personal identifiable information was not collected, nor is it necessary for analysis, and thus will not be used or displayed.\n",
    "\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    ">In the future, when generating the analysis, steps will be taken to ensure the process is well documented and reproducible. These steps will include keeping logs, frequent team communication, and transparency.\n",
    "\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "\n",
    ">Data used is sourced from multiple references, which helps reduce the possibility of discrimination, both directly and through proxy. However, there is concern for bias by income or socioeconomic class when looking at fares and the rider population. \n",
    "\n",
    " - [ ] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    "\n",
    ">Yes, optimized metrics such as fare costs compared to cost of living does not have any apparent unintended effects. Additional metrics can include the local cost of gas, local parking prices, and/or drivability of surrounding areas.\n",
    "\n",
    " - [ ] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [ ] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [ ] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    "\n",
    ">Our results are purely for informational purposes, not designed to be capable of harm. If harm through misrepresentation or misinterpretation were to arise, we can address the concerns on a case by case basis.\n",
    "\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    "\n",
    ">Deletion or unpublication of our model can be easily done if necessary.\n",
    "\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
    "\n",
    ">As our model is purely informational, the primary risk for unintended uses or abuse comes from possible misinterpretations and/or misapplications of our results (such as using our data to justify slowing transportation in lower income neighborhoods). Once deployed, we can monitor the use and citations of our analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Organization*\n",
    "    * Communication will be done over Discord and Imessage. Responses within 24 hours is appreciated, meetings will occur at least once a week either virtually or in person.\n",
    "* *Decision Making*\n",
    "    * A blunt but polite tone will be used within our team, members are expected to treat one another and their ideas with the same care and respect they wish to recieve.\n",
    "    * Decisions will be made by majority vote, though every member's concerns will be considered. Decisions made in constrained time frames or without the input of all team member will be followed up with at a later time, though unilateral decisions are discouraged.\n",
    "* *Contribution*\n",
    "    * Specializations will be sorted when more of the project is flushed out; these specializations will be fluid and dependant on what each member feels most comfortable doing.\n",
    "    * If a member of the group is struggling with something they promised to do, they must reach out to the group at the earliest possible time. Group effort reallocation will be decided on a case by case basis.\n",
    "* *Conflict*\n",
    "    * Communication is greatly encouraged, whether there is a problem with the project, the group, or individual members. This can be done through open conversations with an open mind on the group Discord or Imessage.\n",
    "    * If a problem with a teammate arises, open communication comes first, either one on one or as a group. If problems continue, the professor would be notified."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/2  |  6 PM |  Do background research on potential topics and data sets. | Discuss ideal topic and hypothesis; draft project proposal. | \n",
    "| 2/4  |  6 PM | Work on allocated sections of project proposal. (Marco & Dustin: Background and Prior Work, Aaron: Data, Kera & Amara: Ethics)  | Edit, finalize, and submit Project Proposal. | \n",
    "| 2/11  | 7pm  | Think about possible improvements to project proposal. Find relevant primary and secondary datasets. | Team reflections on project proposal. Updates on found datasets, narrowing down data sets and scope of project.  |\n",
    "| 2/15  | 7pm  | Review project proposal feedback, continue investigation and refinement of data sets.  | Update project proposal according to TA and team feedback. Discuss data expectations and assign roles.  |\n",
    "| 2/16  | 7pm  | Start on individual responsibilities, both in data and project proposal updates. | Progress update on project proposal and data. Finalize updated project proposal. Inspect data structures, types, and potential issues.  |\n",
    "| 2/18  | 6pm  | Work on allocated sections of data checkpoint and roles. | Edit, finalize, and submit data checkpoint. Begin planning EDA and future steps.  |\n",
    "| 2/23  | 6pm  | Begin initial search for correlations; draft initial visualizations, identify potential trends/patterns, initial summary statistics. | Discuss findings, investigate outliers, evaluate initial models. Select model and relevant features to move forward with. Assign initial roles in model making.  |\n",
    "| 2/25  | 7pm  | Start on individual roles. Prepare necessary framework for models, tune parameters, set evaluation criteria. | Updates on model progresses. Redistribution of roles and responsibilities if needed.  |\n",
    "| 3/2  | 6pm  | Continue work on individual roles. Begin working on EDA checkpoint. | Compare performances of different models. Review and finalize model(s).   |\n",
    "| 3/4  | 6pm  | Experiment with different techniques and feature selections. Optimize best performing models. Other fine-tuning to reduce under/overfitting. Continue work on EDA checkpoint. | Assess final models on test set and performance metrics, compare results with baseline. Review, edit, and submit EDA checkpoint. Begin analysis and interpretation of model results, assign roles.  |\n",
    "| 3/9  | 7pm  | Continue assigned analysis and investigations. Note key findings, insights, patterns. Write initial conclusions. | Compare and compile conclusions and analyses. Discuss next steps; address concerns, limitations, or assumptions brought up from analysis.  |\n",
    "| 3/11  | 6pm  | Address last meeting’s concerns; fine tune model, edit analysis as needed. Begin filling in final project. | Discuss final project and video roles and responsibilities based off of work left over. Review, edit, finalize what was already done.  |\n",
    "| 3/16  | 7pm  | Work on individual roles. Identify any possible improvements. | Update on progress of final project and video. Address concerns, ensure project progress is as planned.  |\n",
    "| 3/18  | 6pm  | Finalize individual sections and responsibilities. | Review, edit, and finalize final project and video. Submit!  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
